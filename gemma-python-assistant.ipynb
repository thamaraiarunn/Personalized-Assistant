{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1d9a98",
   "metadata": {
    "papermill": {
     "duration": 0.008287,
     "end_time": "2024-04-07T10:22:16.974473",
     "exception": false,
     "start_time": "2024-04-07T10:22:16.966186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Python Language Assistant Using Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53a6ed",
   "metadata": {
    "papermill": {
     "duration": 0.007506,
     "end_time": "2024-04-07T10:22:16.989975",
     "exception": false,
     "start_time": "2024-04-07T10:22:16.982469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://ai.google.dev/static/site-assets/images/marketing/gemma.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3375e4d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:22:17.009077Z",
     "iopub.status.busy": "2024-04-07T10:22:17.008044Z",
     "iopub.status.idle": "2024-04-07T10:22:32.344746Z",
     "shell.execute_reply": "2024-04-07T10:22:32.343685Z"
    },
    "papermill": {
     "duration": 15.348888,
     "end_time": "2024-04-07T10:22:32.347374",
     "exception": false,
     "start_time": "2024-04-07T10:22:16.998486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 10:22:18.918335: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-07 10:22:18.918471: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-07 10:22:19.063210: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "import keras\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce727c8",
   "metadata": {
    "papermill": {
     "duration": 0.008147,
     "end_time": "2024-04-07T10:22:32.363675",
     "exception": false,
     "start_time": "2024-04-07T10:22:32.355528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Sets environment variables using the `os.environ` dictionary. \n",
    "\n",
    "- `os.environ[\"KERAS_BACKEND\"] = \"jax\"`: This line sets the environment variable `KERAS_BACKEND` to `\"jax\"`. This indicates that Keras, a deep learning library, should use the JAX backend for computation.\n",
    "\n",
    "- `os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"`: This line sets the environment variable `XLA_PYTHON_CLIENT_MEM_FRACTION` to `\"1.00\"`. This environment variable is used by XLA (Accelerated Linear Algebra), a domain-specific compiler for linear algebra operations, to control the fraction of available memory that the XLA Python client will use on a TPU (Tensor Processing Unit).\n",
    "\n",
    "Using these environment variables, the code configures the backend for Keras to use JAX and sets the memory fraction for the XLA Python client to 100%. These configurations are crucial for optimizing performance and memory usage when running deep learning models, especially on TPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d84f0e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:22:32.382736Z",
     "iopub.status.busy": "2024-04-07T10:22:32.382013Z",
     "iopub.status.idle": "2024-04-07T10:22:32.387116Z",
     "shell.execute_reply": "2024-04-07T10:22:32.386162Z"
    },
    "papermill": {
     "duration": 0.016894,
     "end_time": "2024-04-07T10:22:32.389236",
     "exception": false,
     "start_time": "2024-04-07T10:22:32.372342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b45e1f",
   "metadata": {
    "papermill": {
     "duration": 0.00922,
     "end_time": "2024-04-07T10:22:32.407053",
     "exception": false,
     "start_time": "2024-04-07T10:22:32.397833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Initializes a language model object named `gemma_lm` using the GemmaCausalLM class from a library, possibly keras_nlp. It creates the model from a preset configuration named \"gemma_2b_en\". This preset likely contains predefined settings, architecture configurations, and pretrained weights optimized for a specific task or language, in this case, possibly English text generation or understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc4ead3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:22:32.425244Z",
     "iopub.status.busy": "2024-04-07T10:22:32.424812Z",
     "iopub.status.idle": "2024-04-07T10:23:38.431453Z",
     "shell.execute_reply": "2024-04-07T10:23:38.430310Z"
    },
    "papermill": {
     "duration": 66.018767,
     "end_time": "2024-04-07T10:23:38.434248",
     "exception": false,
     "start_time": "2024-04-07T10:22:32.415481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09afe5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:23:38.455047Z",
     "iopub.status.busy": "2024-04-07T10:23:38.454630Z",
     "iopub.status.idle": "2024-04-07T10:23:38.491323Z",
     "shell.execute_reply": "2024-04-07T10:23:38.490283Z"
    },
    "papermill": {
     "duration": 0.049889,
     "end_time": "2024-04-07T10:23:38.493500",
     "exception": false,
     "start_time": "2024-04-07T10:23:38.443611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba58312",
   "metadata": {
    "papermill": {
     "duration": 0.010178,
     "end_time": "2024-04-07T10:23:38.513874",
     "exception": false,
     "start_time": "2024-04-07T10:23:38.503696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This function, get_prompt(query:str)->str, takes a query string as input and returns a prompt string. It formats the prompt using a template string with placeholders for instruction and response. The instruction part is filled with the input query, while the response part is left empty initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a63aa5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:23:38.536322Z",
     "iopub.status.busy": "2024-04-07T10:23:38.535888Z",
     "iopub.status.idle": "2024-04-07T10:23:38.541305Z",
     "shell.execute_reply": "2024-04-07T10:23:38.540261Z"
    },
    "papermill": {
     "duration": 0.019243,
     "end_time": "2024-04-07T10:23:38.543534",
     "exception": false,
     "start_time": "2024-04-07T10:23:38.524291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prompt(query:str)->str:\n",
    "    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "    prompt = template.format(\n",
    "        instruction=query,\n",
    "        response=\"\",\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f3ab3",
   "metadata": {
    "papermill": {
     "duration": 0.010265,
     "end_time": "2024-04-07T10:23:38.564636",
     "exception": false,
     "start_time": "2024-04-07T10:23:38.554371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this code, a TopKSampler object named 'sampler' is created with a parameter k=5 indicating that it will sample from the top 5 likely tokens during text generation. The seed parameter is set to 2 for reproducibility.\n",
    "\n",
    "Then, the Gemma language model 'gemma_lm' is compiled with the sampler object using gemma_lm.compile(sampler=sampler). This likely configures the language model for text generation using the specified sampling strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9998b8cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:23:38.588062Z",
     "iopub.status.busy": "2024-04-07T10:23:38.587110Z",
     "iopub.status.idle": "2024-04-07T10:23:38.600633Z",
     "shell.execute_reply": "2024-04-07T10:23:38.599556Z"
    },
    "papermill": {
     "duration": 0.027765,
     "end_time": "2024-04-07T10:23:38.603006",
     "exception": false,
     "start_time": "2024-04-07T10:23:38.575241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
    "gemma_lm.compile(sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376dd36",
   "metadata": {
    "papermill": {
     "duration": 0.010425,
     "end_time": "2024-04-07T10:23:38.624035",
     "exception": false,
     "start_time": "2024-04-07T10:23:38.613610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing Before Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56757158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:23:38.646892Z",
     "iopub.status.busy": "2024-04-07T10:23:38.646201Z",
     "iopub.status.idle": "2024-04-07T10:24:16.879959Z",
     "shell.execute_reply": "2024-04-07T10:24:16.878849Z"
    },
    "papermill": {
     "duration": 38.258735,
     "end_time": "2024-04-07T10:24:16.893200",
     "exception": false,
     "start_time": "2024-04-07T10:23:38.634465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712485444.866398      25 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "W0000 00:00:1712485444.941388      25 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
      "W0000 00:00:1712485445.205555      25 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "What are list comprehensions in Python?\n",
      "\n",
      "Response:\n",
      "List comprehensions are a Python feature that allow you to generate a\n",
      "list from a list comprehension.\n",
      "\n",
      "Syntax:\n",
      "\n",
      "list_comprehension = [expression for expression in iterable]\n",
      "\n",
      "Example:\n",
      "What is list comprehensions and when to use it?\n",
      "\n",
      "Response:\n",
      "The list comprehension is a concise way to build a new list from an\n",
      "iterable (a sequence or a generator).\n",
      "\n",
      "Example:\n",
      "\n",
      "# List Comprehension\n",
      "# This code creates a new list using the list comprehension\n",
      "# syntax\n",
      "# [item * 2 for item in [1,2,3,4,5,6,7,8,9]]\n",
      "\n",
      "# The result of this operation would be\n",
      "[item * 2 for item in [1,2,3,4,5,6,7,8,9]] = [2,4,6,8,10,12,14,16,18]\n",
      "\n",
      "# List Comprehension with a list\n",
      "# In this case we have an iterable which is a list,\n",
      "# we can use list comprehension to create a new list.\n",
      "# The result of this operation would be\n",
      "# [item * 2 for item in [2,4,6,8,10,12,14,16,18,20]] = [4,8,12,16,20,24,28,32,36,40]\n",
      "\n",
      "# List Comprehension using a dictionary\n",
      "# Here we have a dictionary,\n",
      "# we can use list comprehension to create a new dictionary\n",
      "# The result of this operation would be\n",
      "# {key: value * 2 for key, value in [('a', 1), ('b', 2), ('c', 3), ('d', 4)]} = {('a', 2), ('b', 4), ('c', 6), ('d', 8)}\n",
      "\n",
      "# List Comprehension Using a generator\n",
      "# Here we have a generator,\n",
      "# we can use list comprehension to create a new generator\n",
      "# The result of this operation would be\n",
      "# (key * 2 for key in [2, 4, 6, 8, 10, 12, 14, 16, 18]) = 2, 4, 6, 8, 1\n"
     ]
    }
   ],
   "source": [
    "prompt = get_prompt(\"What are list comprehensions in Python?\")\n",
    "print(gemma_lm.generate(prompt, max_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afe2e20d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:24:16.917740Z",
     "iopub.status.busy": "2024-04-07T10:24:16.916898Z",
     "iopub.status.idle": "2024-04-07T10:24:18.708116Z",
     "shell.execute_reply": "2024-04-07T10:24:18.706836Z"
    },
    "papermill": {
     "duration": 1.806426,
     "end_time": "2024-04-07T10:24:18.710676",
     "exception": false,
     "start_time": "2024-04-07T10:24:16.904250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "How to implement a stack in Python?\n",
      "\n",
      "Response:\n",
      "The implementation of a stack in Python is straightforward. We can implement it using a list.\n",
      "\n",
      "Here is the list of steps:\n",
      "\n",
      "1. Create a list called stack.\n",
      "\n",
      "2. Define an empty stack.\n",
      "\n",
      "3. Push the element onto the list.\n",
      "\n",
      "4. Pop an element off the list.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = get_prompt(\"How to implement a stack in Python?\")\n",
    "print(gemma_lm.generate(prompt, max_length=512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80ccd5",
   "metadata": {
    "papermill": {
     "duration": 0.010042,
     "end_time": "2024-04-07T10:24:18.731455",
     "exception": false,
     "start_time": "2024-04-07T10:24:18.721413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reading Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80cf0027",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:24:18.753658Z",
     "iopub.status.busy": "2024-04-07T10:24:18.753269Z",
     "iopub.status.idle": "2024-04-07T10:24:56.395699Z",
     "shell.execute_reply": "2024-04-07T10:24:56.394788Z"
    },
    "papermill": {
     "duration": 37.65683,
     "end_time": "2024-04-07T10:24:56.398541",
     "exception": false,
     "start_time": "2024-04-07T10:24:18.741711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#questions table\n",
    "df_questions = pd.read_csv('../input/pythonquestions/Questions.csv',\n",
    "                            encoding = \"ISO-8859-1\",\n",
    "                            usecols = ['Id','Score','Title'])\n",
    "#answers table\n",
    "df_answers = pd.read_csv('../input/pythonquestions/Answers.csv',\n",
    "                            encoding = \"ISO-8859-1\",\n",
    "                            usecols = ['ParentId','Score','Body'],#parent id links to the questions table\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7bfe91",
   "metadata": {
    "papermill": {
     "duration": 0.010023,
     "end_time": "2024-04-07T10:24:56.418835",
     "exception": false,
     "start_time": "2024-04-07T10:24:56.408812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sorting for threshold score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e31573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:24:56.441064Z",
     "iopub.status.busy": "2024-04-07T10:24:56.440083Z",
     "iopub.status.idle": "2024-04-07T10:24:56.484309Z",
     "shell.execute_reply": "2024-04-07T10:24:56.483418Z"
    },
    "papermill": {
     "duration": 0.05798,
     "end_time": "2024-04-07T10:24:56.486666",
     "exception": false,
     "start_time": "2024-04-07T10:24:56.428686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_questions = df_questions[df_questions['Score'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3e30ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:24:56.508979Z",
     "iopub.status.busy": "2024-04-07T10:24:56.508267Z",
     "iopub.status.idle": "2024-04-07T10:24:56.813342Z",
     "shell.execute_reply": "2024-04-07T10:24:56.812272Z"
    },
    "papermill": {
     "duration": 0.319066,
     "end_time": "2024-04-07T10:24:56.815882",
     "exception": false,
     "start_time": "2024-04-07T10:24:56.496816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_answers = df_answers[df_answers['Score'] > 0]\\\n",
    "    .sort_values('Score',ascending=False)\\\n",
    "    .drop_duplicates(subset=['ParentId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972ceab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:24:56.839482Z",
     "iopub.status.busy": "2024-04-07T10:24:56.838514Z",
     "iopub.status.idle": "2024-04-07T10:24:57.095636Z",
     "shell.execute_reply": "2024-04-07T10:24:57.094509Z"
    },
    "papermill": {
     "duration": 0.272104,
     "end_time": "2024-04-07T10:24:57.098277",
     "exception": false,
     "start_time": "2024-04-07T10:24:56.826173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = df_questions.merge(df_answers,left_on = 'Id', right_on = 'ParentId')\\\n",
    "    .rename(columns={'Title':'Question','Body':'Answer'})[['Question','Answer','Score_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e80625f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:24:57.121198Z",
     "iopub.status.busy": "2024-04-07T10:24:57.120778Z",
     "iopub.status.idle": "2024-04-07T10:24:57.174488Z",
     "shell.execute_reply": "2024-04-07T10:24:57.173604Z"
    },
    "papermill": {
     "duration": 0.06822,
     "end_time": "2024-04-07T10:24:57.176947",
     "exception": false,
     "start_time": "2024-04-07T10:24:57.108727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = qa.sort_values(\"Score_x\",ascending=False).head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4a30a7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:24:57.199943Z",
     "iopub.status.busy": "2024-04-07T10:24:57.199157Z",
     "iopub.status.idle": "2024-04-07T10:24:57.270756Z",
     "shell.execute_reply": "2024-04-07T10:24:57.269630Z"
    },
    "papermill": {
     "duration": 0.086,
     "end_time": "2024-04-07T10:24:57.273270",
     "exception": false,
     "start_time": "2024-04-07T10:24:57.187270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "for index, row in qa.iterrows():\n",
    "    train.append(f\"Question:\\n{row['Question']}\\n\\nAnswer:\\n{row['Answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c5d58b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:24:57.296296Z",
     "iopub.status.busy": "2024-04-07T10:24:57.295389Z",
     "iopub.status.idle": "2024-04-07T10:24:57.720326Z",
     "shell.execute_reply": "2024-04-07T10:24:57.719136Z"
    },
    "papermill": {
     "duration": 0.438977,
     "end_time": "2024-04-07T10:24:57.722975",
     "exception": false,
     "start_time": "2024-04-07T10:24:57.283998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_lm.backbone.enable_lora(rank=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fd02a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:24:57.745360Z",
     "iopub.status.busy": "2024-04-07T10:24:57.744614Z",
     "iopub.status.idle": "2024-04-07T10:24:57.780317Z",
     "shell.execute_reply": "2024-04-07T10:24:57.779349Z"
    },
    "papermill": {
     "duration": 0.048997,
     "end_time": "2024-04-07T10:24:57.782352",
     "exception": false,
     "start_time": "2024-04-07T10:24:57.733355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cbdf78",
   "metadata": {
    "papermill": {
     "duration": 0.011341,
     "end_time": "2024-04-07T10:24:57.804737",
     "exception": false,
     "start_time": "2024-04-07T10:24:57.793396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fine tuning using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a649ceac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:24:57.829595Z",
     "iopub.status.busy": "2024-04-07T10:24:57.828673Z",
     "iopub.status.idle": "2024-04-07T10:29:11.229745Z",
     "shell.execute_reply": "2024-04-07T10:29:11.228679Z"
    },
    "papermill": {
     "duration": 253.416148,
     "end_time": "2024-04-07T10:29:11.231980",
     "exception": false,
     "start_time": "2024-04-07T10:24:57.815832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1712485544.535417      69 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 207ms/step - loss: 1.5426 - sparse_categorical_accuracy: 0.6544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7e2eac3866e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the input sequence length to 128 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = 128\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "gemma_lm.fit(train, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e948537b",
   "metadata": {
    "papermill": {
     "duration": 0.099409,
     "end_time": "2024-04-07T10:29:11.429498",
     "exception": false,
     "start_time": "2024-04-07T10:29:11.330089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing after tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05fe8418",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:29:11.626946Z",
     "iopub.status.busy": "2024-04-07T10:29:11.626553Z",
     "iopub.status.idle": "2024-04-07T10:29:40.527981Z",
     "shell.execute_reply": "2024-04-07T10:29:40.526661Z"
    },
    "papermill": {
     "duration": 29.00281,
     "end_time": "2024-04-07T10:29:40.530308",
     "exception": false,
     "start_time": "2024-04-07T10:29:11.527498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1712485777.265228      25 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
      "W0000 00:00:1712485777.540651      25 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "What are list comprehensions in Python?\n",
      "\n",
      "Response:\n",
      "List comprehensions are a concise way of writing a list by specifying a function to apply to each element in the list.\n",
      "\n",
      "The syntax is similar to the following:\n",
      "\n",
      "list comprehension = for item in iterable: expression\n",
      "\n",
      "In this example, the function is the one-liner\n",
      "\n",
      "print \"The sum is %d\" % (sum(iterable),)\n",
      "\n",
      "The result is the same as:\n",
      "\n",
      "list = []\n",
      "for item in iterable:\n",
      "list.append(expression)\n",
      "print \"The sum is %d\" % (sum(list),)\n"
     ]
    }
   ],
   "source": [
    "prompt = get_prompt(\"What are list comprehensions in Python?\")\n",
    "print(gemma_lm.generate(prompt, max_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e194fb7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T10:29:40.738973Z",
     "iopub.status.busy": "2024-04-07T10:29:40.738120Z",
     "iopub.status.idle": "2024-04-07T10:29:42.675478Z",
     "shell.execute_reply": "2024-04-07T10:29:42.674142Z"
    },
    "papermill": {
     "duration": 2.04093,
     "end_time": "2024-04-07T10:29:42.678009",
     "exception": false,
     "start_time": "2024-04-07T10:29:40.637079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "How to implement a stack in Python?\n",
      "\n",
      "Response:\n",
      "You could use a list, or better, a class, to implement the stack. Here's an example:\n",
      "\n",
      "class Stack():\n",
      "\n",
      "    def __init__(self):\n",
      "        self._data = []\n",
      "\n",
      "    def push(self, item):\n",
      "        self._data.append(item)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = get_prompt(\"How to implement a stack in Python?\")\n",
    "print(gemma_lm.generate(prompt, max_length=512))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 262,
     "sourceId": 726715,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5171,
     "sourceId": 11371,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 452.620257,
   "end_time": "2024-04-07T10:29:46.375868",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-07T10:22:13.755611",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
